---
layout: post
categories:
  - thesis
  - augmented reality
---
Calibration between the IMU and camera is essential, but there are multiple methods to achieve this. A few references are listed.

Toby and I found several articles on visual-inertial calibration. [Horn](#horn1987closed) is the "original one", apparently coming up with the need for this system.  The work by [Lobo and Dias](#lobo2007relative) extends this, but needs to place one of two sensors at the center of a turntable.  [Hol et al. '08](#hol2008new) proposes a method where this extra tool is not needed.  A less dense description can be found in [Hol](#hol2011sensor).

Today I also found an article of [Hol et al. '06](#hol2006sensor) about sensor fusion for AR, between inertial and visual sensors.  They fuse the readings with an EKF.  It is unclear if they include the accelerometer and gyroscope readings in the state vector, so they can predict them.  In their conclusion section, they suggest for further investigation in 'design of accurate self-calibration methods, including uncertainty measures for the computer vision measurements'.  As my work won't be really original (it's mainly a different localization module for the camera), I might have to look more into calibration?

[Hol et al. '06](#hol2006) also uses [FREE-D](#thomas1997free-d), 'a conventional AR-tracking system requiring a heavy infrastructure (lots of markers on the ceiling)' which can be used as a ground truth system.  I haven't investigated this too much yet, but it sounds like it might be outdated (1997) by now.

{% include bibliography.html keys="horn1987closed,hol2006sensor,hol2008new,hol2011sensor,lobo2007relative,thomas1997free-d" %}
