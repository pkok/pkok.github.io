---
layout: post
categories:
  - thesis
---

Decided to focus on a simulated implementation for my thesis. Main reason: could not obtain extrinsic camera-IMU calibration. 

## Reason for switch to simulation
Toby and I decided that, for now, I will focus on an implementation on a simulator for the system.  I could not obtain an extrinsic camera-IMU calibration.  The things I have tried:

1. I tried [Furgale et al.](#furgale2013unified)'s [Kalibr](https://github.com/ethz-asl/kalibr), but it didn't work (see [this](/2015/02/17) and [this](/2015/02/18) post; setting the exposure to 400 microseconds did not resolve the issue);
2. The method of [Lobo and Dias](#lobo2007relative) is named [InerVis](http://www2.deec.uc.pt/~jlobo/jlobo/InerVis/InerVis_Toolbox.html), but the download section is down.
3. When I contacted [Kelly and Sukhatme](#kelly2011visualinertial), they adviced me to use Kalibr as their toolkit is slow.
4. [Mirzaei and Roumeliotis](#mirzaei2008kfbased)'s and [Hol](#hol2011sensor)'s implementations cannot be shared as the suite or parts thereof are under a commercial license.  Hol told me that, by following his [dissertation](#hol2011sensor), it should not be too hard to implement this Kalman filter based approach myself.
5. Toby and I decided that up to now I have spend enough time on this issue, and should continue.

## Simulator choice
There are basically two options: Gazebo and USARsim. 

Pro's of Gazebo vs. USARsim:

- Should link easily with ROS
- Behaviour of objects can be "scripted" in C++, can be used under Windows and Linux. USARsim works with C# and needs Windows.

Pro's of USARsim vs. Gazebo:

- Known to be very detailed, as it is based on the Unreal Tournament Engine.
- Arnoud has experience with USARsim, and I could ask for tips very directly.
- Highly detailed textures, which make it visually more realistic. This could be used for camera localisation. However, we will most likely still use the marker based method.

Cons of simulating vs. the real world:

- No model of motion blur.
- No validated model of IMU.
- No validated model of camera.

I will try to talk with Arnoud tomorrow to make a decision on which simulator to use.

## Motion blur model
Motion blur will be simulated in the following way. Let $$F_t$$ be the camera frame at the current time $$t$$. The blurred frame $$F^B_t$$ is obtained as the linearly weighed combination of the previously blurred frame $$F^B_{t-1}$$ and $$F_t$$ as $$F^B_t = (1 - w) F^B_{t-1} + w F_t$$. For the initial blurred image $$F^B_0$$, take $$w = 1$$, or for $$t \leq 0$$, take $$F^B_{t-1} = F_0$$.

By expanding this recursive expression once for an arbitrary time $$t$$, you have $$F^B_t = \left(1 - w\right) \left( \left(1 - w\right) F^B_{t-2} + w F_{t-1}\right) + w F_t = (1 - 2w + w^2) F^B_{t-2} + (w - w^2) F_{t-1} + w F_t$$.  **Does this imply some stronger/deeper connection? Didn't Sutton and Barto have to say something about this in their chapter on TD(Î»)?**

Bigger values for the weight $$w$$ will reduce the blur, which corresponds with a lower exposure time in real cameras. Lower values for the weight $$w$$ will increase the blur more, as if the exposure time has been increased. However, the difference with real motion blur is that in this model a discrete path is recorded of the movement. A higher framerate will make this model more realistic.

More complex and realistic models exist.  Some need the 3D structure of the observed scene ([Potmesil and Chakravarty](#potmesil1983modeling)), while others are designed with stop-motion animations in mind ([Brostow and Essa](#brostow2001image)).  For a full overview, see [Navarro et al.](#navarro2011motionblur).

These models have not been implemented for the sake of time restrictions. The proposed model does seem sufficient for the current purpose.

## Ideas about experiments
The [GP-BayesFilter](#ko2009gp-bayesfilter) framework, and especially [GPBF-Learn](#ko2011gpbf-learn), is designed as an alternative for [GP Latent Variable Models](#lawrence2003gplvm) which exploit the information provided by a system's control vector (input for state change).

To see if the control vector in the localisation problem of AR is informative, we can compare the error of GPBF-Learn and GPLVM on the same Kalman filter $$KF^\mathbf{u}$$ and data set $$D$$.  This "difference in errors" $$\mathbf{e}_\mathbf{u} = \mathbf{e}^\mbox{GPBF-Learn}_\mbox{GPLVM}(KF^\mathbf{u})$$ (**needs to be defined more precisely**) should be compared with a similar Kalman filter, but not using any control vector, $$KF^\emptyset$$, and the same dat set $$D$$.  This also results in a "difference in errors" $$\mathbf{e}_\emptyset = \mathbf{e}^\mbox{GPBF-Learn}_\mbox{GPLVM}(KF^\emptyset)$$.  Comparing these two "differences in errors" can bring us to several conclusions:

1. If $$\mathbf{e}_\mathbf{u} < \mathbf{e}_\emptyset$$, the control vector is informative.
2. If $$\mathbf{e}_\mathbf{u} \approx \mathbf{e}_\emptyset$$, the control vector is not informative.
3. If $$\mathbf{e}_\mathbf{u} > \mathbf{e}_\emptyset$$, the control vector contains misleading information.

[Bleser et al.](#bleser2008advanced) defines several Kalman filters for the earlier described localisation problem.  For these experiments, their **first/third?** and fourth models will be used.  The former model does not use any control vector, whereas the other does use the accelerometer readings in the control vector.  **Are these models comparative?**

{% include bibliography.html keys="bleser2008advanced,furgale2013unified,lobo2007relative,mirzaei2008kfbased,kelly2011visualinertial,hol2011sensor,lawrence2003gplvm,ko2009gp-bayesfilter,ko2011gpbf-learn,potmesil1983modeling,brostow2001image,navarro2011motionblur" %}
